{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from avalanche.benchmarks import RotatedMNIST\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics, forgetting_metrics\n",
    "from avalanche.logging import InteractiveLogger, TextLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "EPOCHS = 4\n",
    "INDEP_THRESHOLD = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_resnet18_for_mnist():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_active_task_vectors(n_experiences=7, rotations_list=[0, 15, 30, 45, 60, 75, 90]):\n",
    "  mnist_transform = Compose([Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "  rotated_benchmark = RotatedMNIST(\n",
    "      n_experiences=n_experiences, seed=42, rotations_list=rotations_list,\n",
    "      dataset_root=\"./data\",\n",
    "      train_transform=mnist_transform,\n",
    "      eval_transform=mnist_transform)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model_base = modify_resnet18_for_mnist()\n",
    "  model_base = model_base.to(device)\n",
    "  trainer = Naive(\n",
    "      model_base,\n",
    "      optimizer=torch.optim.SGD(model_base.parameters(), lr=0.01, momentum=0.9),\n",
    "      criterion=torch.nn.CrossEntropyLoss(),\n",
    "      train_mb_size=128,\n",
    "      device=device,\n",
    "      train_epochs=5,\n",
    "      evaluator=EvaluationPlugin(\n",
    "          accuracy_metrics(epoch=True, stream=True),\n",
    "          loggers=[InteractiveLogger()]))\n",
    "\n",
    "  task_vectors = []\n",
    "  for experience in rotated_benchmark.train_stream:\n",
    "      model_tuned = modify_resnet18_for_mnist()\n",
    "      model_tuned = model_tuned.to(device)\n",
    "      model_tuned.load_state_dict(model_base.state_dict())\n",
    "      trainer = Naive(\n",
    "          model_tuned,\n",
    "          optimizer=torch.optim.SGD(model_tuned.parameters(), lr=0.01, momentum=0.9),\n",
    "          criterion=torch.nn.CrossEntropyLoss(),\n",
    "          train_mb_size=128,\n",
    "          device=device,\n",
    "          train_epochs=5,\n",
    "          evaluator=EvaluationPlugin(\n",
    "              accuracy_metrics(epoch=True, stream=True),\n",
    "              loggers=[InteractiveLogger()]))\n",
    "      trainer.train(experience)\n",
    "      task_vector = []\n",
    "      for p_base, p_tuned in zip(model_base.parameters(), model_tuned.parameters()):\n",
    "          task_vector.append((p_tuned.data - p_base.data).detach().cpu().numpy())\n",
    "      task_vector = np.concatenate([p.flatten() for p in task_vector])\n",
    "      task_vectors.append(task_vector)\n",
    "\n",
    "  return task_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint_model_on_mnist(benchmark, device, epochs=5):\n",
    "    print(\"\\n### Training Joint Model on Shuffled MNIST Tasks ###\")\n",
    "    joint_model = modify_resnet18_for_mnist().to(device)\n",
    "    optimizer = optim.SGD(joint_model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(accuracy_metrics(stream=True), loss_metrics(stream=True), loggers=[logger])\n",
    "\n",
    "    trainer = Naive(joint_model, optimizer, criterion, train_mb_size=128, device=device, evaluator=eval_plugin)\n",
    "    all_train_data = ConcatDataset([experience.dataset for experience in benchmark.train_stream])\n",
    "\n",
    "    # Get all sample indices and shuffle them\n",
    "    all_indices = np.arange(len(all_train_data))\n",
    "    np.random.shuffle(all_indices)\n",
    "    shuffled_train_data = Subset(all_train_data, all_indices)\n",
    "    shuffled_train_loader = DataLoader(shuffled_train_data, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Manually train the joint model using the shuffled DataLoader\n",
    "    for epoch in range(epochs):\n",
    "        for batch in shuffled_train_loader:\n",
    "            inputs, targets = batch[0], batch[1]\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = joint_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return joint_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_task_vector(pretrained_model, finetuned_model):\n",
    "    task_vector = []\n",
    "    param_shapes = []  # Store parameter shapes\n",
    "\n",
    "    for p_pre, p_fine in zip(pretrained_model.parameters(), finetuned_model.parameters()):\n",
    "        param_shapes.append(p_pre.shape)  # Save the shape of each parameter\n",
    "        task_vector.append((p_fine.data - p_pre.data).detach().cpu().numpy())\n",
    "\n",
    "    flattened_task_vector = np.concatenate([p.flatten() for p in task_vector])\n",
    "    return flattened_task_vector, param_shapes\n",
    "\n",
    "def is_approximately_independent(vector, active_vectors, device='cuda'):\n",
    "    vector = torch.tensor(vector, device=device, dtype=torch.float32)\n",
    "    active_vectors = torch.tensor(active_vectors, device=device, dtype=torch.float32)\n",
    "    dim = vector.shape[0]\n",
    "    threshold = INDEP_THRESHOLD / torch.sqrt(torch.tensor(dim, dtype=torch.float32, device=device))\n",
    "    total_norm = torch.norm(vector)\n",
    "    U, _, _ = torch.linalg.svd(active_vectors.T, full_matrices=False)\n",
    "    projection = U @ (U.T @ vector)\n",
    "    normalized_projection_magnitude = torch.norm(projection) / total_norm\n",
    "    is_independent = normalized_projection_magnitude < threshold\n",
    "    return is_independent, normalized_projection_magnitude.item(), threshold.item(), projection.cpu().numpy()\n",
    "\n",
    "def localize_and_stitch(model, pretrained_model, task_vector, param_shapes, all_masks, sparsity=0.01):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    abs_vector = torch.abs(torch.tensor(task_vector))   # Create a sparse mask\n",
    "    k = int(sparsity * abs_vector.numel())  # Determine number of top-k elements\n",
    "    topk_indices = abs_vector.topk(k).indices  # Get indices of top-k elements\n",
    "    mask = torch.zeros_like(abs_vector)  # Initialize a zero mask\n",
    "    mask[topk_indices] = 1  # Set top-k indices to 1 for the mask\n",
    "\n",
    "    combined_masks = []\n",
    "    for m in all_masks:\n",
    "        if not isinstance(m, torch.Tensor):\n",
    "          m = torch.tensor(m, device=device, dtype=torch.float32)\n",
    "          combined_masks.append(m)\n",
    "    combined_masks.append(mask)\n",
    "\n",
    "    all_masks_tensor = torch.stack(combined_masks, dim=1)  # shape: (flattened_dim, num_masks)\n",
    "\n",
    "    counts = torch.sum(all_masks_tensor, dim=1)  # shape: (flattened_dim,)\n",
    "\n",
    "\n",
    "    stitched_mask = torch.where(\n",
    "        mask > 0,\n",
    "        torch.where(counts > 0, 1.0 / counts, torch.zeros_like(counts)),\n",
    "        torch.zeros_like(counts)\n",
    "    )\n",
    "\n",
    "    offset = 0\n",
    "    flat_task_vector = torch.tensor(task_vector, device=device)\n",
    "    for param, shape in zip(model.parameters(), param_shapes):\n",
    "        numel = np.prod(shape)\n",
    "        task_slice = flat_task_vector[offset:offset + numel].view(shape)\n",
    "        mask_slice = stitched_mask[offset:offset + numel].view(shape)\n",
    "        param.data += (task_slice * mask_slice).to(param.device)\n",
    "        offset += numel\n",
    "    return model, mask.detach().cpu()\n",
    "\n",
    "def continual_learning_with_localize_and_stitch(rotated_benchmark, random_seed=1234, use_localize_and_stitch=True, task_vectors_active=None, all_masks=[]):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_base = modify_resnet18_for_mnist()\n",
    "    model_base = model_base.to(device)\n",
    "    optimizer = torch.optim.SGD(model_base.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model_pretrained = modify_resnet18_for_mnist()\n",
    "    model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, 10)  # Adjust for 10 classes\n",
    "    model_pretrained.load_state_dict(model_base.state_dict())\n",
    "    model_pretrained = model_pretrained.to(device)\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(experience=True),\n",
    "        loggers=[InteractiveLogger()])\n",
    "\n",
    "    current_experience_accuracies = []\n",
    "    num_dependent_tasks = 0\n",
    "    for task_id, experience in enumerate(rotated_benchmark.train_stream):\n",
    "        print(f\"\\n### Training on Task {task_id+1} ###\")\n",
    "\n",
    "        model_finetuned = modify_resnet18_for_mnist()\n",
    "        model_finetuned.load_state_dict(model_pretrained.state_dict())\n",
    "        model_finetuned = model_finetuned.to(device)\n",
    "\n",
    "        trainer = Naive(model_finetuned, optimizer, criterion, train_mb_size=128, device=device, evaluator=eval_plugin, train_epochs=EPOCHS)\n",
    "        trainer.train(experience)\n",
    "        task_vector, param_shapes = compute_task_vector(model_pretrained, model_finetuned)\n",
    "\n",
    "        if use_localize_and_stitch:\n",
    "          independent, projection_magnitude, _, projection = is_approximately_independent(task_vector, task_vectors_active)\n",
    "          if independent:\n",
    "              task_vectors_active.append(task_vector)\n",
    "              model_base, new_mask = localize_and_stitch(\n",
    "                  model=model_base,\n",
    "                  pretrained_model=model_pretrained,\n",
    "                  task_vector=task_vector,\n",
    "                  param_shapes=param_shapes,\n",
    "                  all_masks=all_masks,\n",
    "                  sparsity=0.0)\n",
    "              all_masks.append(new_mask)\n",
    "          else:\n",
    "              num_dependent_tasks += 1\n",
    "              model_base, new_mask = localize_and_stitch(\n",
    "                  model=model_base,\n",
    "                  pretrained_model=model_pretrained,\n",
    "                  task_vector=projection,\n",
    "                  param_shapes=param_shapes,                  \n",
    "                  all_masks=all_masks,                  \n",
    "                  sparsity=0.01)\n",
    "              all_masks.append(new_mask)\n",
    "\n",
    "        trainer.eval(rotated_benchmark.test_stream)\n",
    "        metrics = eval_plugin.get_last_metrics()\n",
    "        acc = [v for k, v in metrics.items() if \"Top1_Acc_Exp/eval_phase/test_stream\" in k][task_id]\n",
    "        current_experience_accuracies.append(acc)\n",
    "        print(f\"\\n### Accuracies {acc} ###\")\n",
    "    return current_experience_accuracies, num_dependent_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotations_list(total_var, num_experiences, theta_0=None):\n",
    "    if theta_0 is None:\n",
    "        theta_0 = random.randint(0, 180)  # Randomly initialize starting angle\n",
    "    rotations = [theta_0]\n",
    "    for _ in range(num_experiences - 1):\n",
    "        R = random.randint(1, total_var)  # Random step size within total variation\n",
    "        theta_next = (rotations[-1] + R) % 180\n",
    "        rotations.append(theta_next)\n",
    "    return rotations\n",
    "\n",
    "def perform_ablation_study():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_test_experiences_list = [5, 10]\n",
    "    total_variations = [5, 10, 30, 60]\n",
    "    expressivity_scenarios = [\n",
    "        [10, 20, 30, 40]\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    results_txt_path = \"ablation_study_results.txt\"\n",
    "\n",
    "    with open(results_txt_path, \"w\") as txt_file:\n",
    "        txt_file.write(\"Ablation Study Results\\n\")\n",
    "        txt_file.write(\"=\" * 50 + \"\\n\")\n",
    "        txt_file.flush()\n",
    "\n",
    "        for expressivity in expressivity_scenarios:\n",
    "            # Generate active task vector set based on expressivity\n",
    "            print(f\"Generating active task vector set for expressivity: {expressivity}\")\n",
    "            txt_file.write(f\"Generating active task vector set for expressivity: {expressivity}\\n\")\n",
    "            txt_file.flush()\n",
    "            active_rotation_angles = expressivity\n",
    "            task_vectors_active = generate_active_task_vectors(\n",
    "                n_experiences=len(active_rotation_angles),\n",
    "                rotations_list=active_rotation_angles,\n",
    "            )\n",
    "\n",
    "            for num_test_experiences in num_test_experiences_list:\n",
    "                for total_var in total_variations:\n",
    "                    test_description = f\"\\n### Testing with T={num_test_experiences}, TV={total_var}, Expressivity={expressivity} ###\"\n",
    "                    print(test_description)\n",
    "                    txt_file.write(test_description + \"\\n\")\n",
    "                    txt_file.flush()\n",
    "\n",
    "                    # Generate rotation angles and test experiences\n",
    "                    rotation_angles = generate_rotations_list(total_var, num_test_experiences)\n",
    "                    rotated_benchmark = RotatedMNIST(\n",
    "                        n_experiences=num_test_experiences,\n",
    "                        seed=42,\n",
    "                        rotations_list=rotation_angles,\n",
    "                        return_task_id=True,\n",
    "                    )\n",
    "\n",
    "                    # Train a joint model on the test experiences\n",
    "                    joint_model = train_joint_model_on_mnist(rotated_benchmark, device, epochs=5)\n",
    "                    joint_model.eval()\n",
    "\n",
    "                    eval_plugin = EvaluationPlugin(\n",
    "                        accuracy_metrics(experience=True, epoch=True),\n",
    "                        loggers=[InteractiveLogger()],\n",
    "                    )\n",
    "                    trainer = Naive(\n",
    "                        joint_model,\n",
    "                        optimizer=torch.optim.SGD(joint_model.parameters(), lr=0.01),\n",
    "                        criterion=torch.nn.CrossEntropyLoss(),\n",
    "                        train_mb_size=128,\n",
    "                        device=device,\n",
    "                        evaluator=eval_plugin,\n",
    "                    )\n",
    "                    trainer.eval(rotated_benchmark.train_stream)\n",
    "                    joint_model_metrics = eval_plugin.get_last_metrics()\n",
    "                    joint_accuracies = [\n",
    "                        v for k, v in joint_model_metrics.items() if \"Top1_Acc_Exp/eval_phase/train_stream\" in k\n",
    "                    ]\n",
    "\n",
    "                    print(\"Joint Accuracies:\", joint_accuracies)\n",
    "\n",
    "                    # Run continual learning with Localize and Stitch\n",
    "                    accs_localize, num_dependent_tasks_localize = continual_learning_with_localize_and_stitch(\n",
    "                        task_vectors_active=task_vectors_active,\n",
    "                        rotated_benchmark=rotated_benchmark,\n",
    "                        random_seed=42,\n",
    "                        use_localize_and_stitch=True,\n",
    "                    )\n",
    "\n",
    "                    # Run continual learning without Localize and Stitch\n",
    "                    accs_finetune, num_dependent_tasks_finetune = continual_learning_with_localize_and_stitch(\n",
    "                        rotated_benchmark=rotated_benchmark,\n",
    "                        random_seed=42,\n",
    "                        use_localize_and_stitch=False,\n",
    "                    )\n",
    "\n",
    "                    # Collect results\n",
    "                    result_entry = {\n",
    "                        \"T\": num_test_experiences,\n",
    "                        \"TV\": total_var,\n",
    "                        \"Expressivity\": expressivity,\n",
    "                        \"Dependent_Tasks_Localize\": num_dependent_tasks_localize,\n",
    "                        \"Dependent_Tasks_Finetune\": num_dependent_tasks_finetune,\n",
    "                        \"Accs_Localize\": accs_localize,\n",
    "                        \"Accs_Finetune\": accs_finetune,\n",
    "                        \"Accuracy_Joint\": joint_accuracies\n",
    "                    }\n",
    "                    results.append(result_entry)\n",
    "\n",
    "                    # Write results to the text file\n",
    "                    txt_file.write(f\"Dependent Tasks (Localize & Stitch): {num_dependent_tasks_localize}\\n\")\n",
    "                    txt_file.write(f\"Dependent Tasks (Fine-Tune): {num_dependent_tasks_finetune}\\n\")\n",
    "                    txt_file.write(f\"Accuracy (Localize & Stitch): {accs_localize}\\n\")\n",
    "                    txt_file.write(f\"Accuracy (Fine-Tune): {accs_finetune}\\n\")\n",
    "                    txt_file.write(f\"Accuracy (Joint): {joint_accuracies}\\n\")\n",
    "                    txt_file.write(\"-\" * 50 + \"\\n\")\n",
    "                    txt_file.flush()\n",
    "\n",
    "    # Save results to a file for further analysis\n",
    "    results_path = \"ablation_study_results.pkl\"\n",
    "    with open(results_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"\\n### Ablation study results saved to {results_path} ###\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating active task vector set for expressivity: [10, 20, 30, 40]\n",
      "-- >> Start of training phase << --\n",
      "  1%|          | 5/469 [07:53<12:16:56, 95.29s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    results = perform_ablation_study()\n",
    "    # Analyze results (e.g., plot accuracies, regret, or dependency trends)\n",
    "    for result in results:\n",
    "        print(f\"T={result['T']}, TV={result['TV']}, Expressivity={result['Expressivity']}\")\n",
    "        print(f\"Accs Localize: {result['Accs_Localize']}\")\n",
    "        print(f\"Accs Finetune: {result['Accs_Finetune']}\")\n",
    "        print(f\"Dependent Tasks Localize: {result['Dependent_Tasks_Localize']}\")\n",
    "        print(f\"Dependent Tasks Finetune: {result['Dependent_Tasks_Finetune']}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
